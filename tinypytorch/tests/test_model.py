# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_tests.test_model.ipynb.

# %% auto 0
__all__ = ['A', 'B', 'test_mse_should_return_true', 'test_mse_gradient_should_return_true', 'test_log_softmax_should_return_true',
           'test_nll_should_return_true', 'test_cross_entropy_loss', 'test_relu_should_return_true',
           'test_relu_gradient_should_return_true', 'test_intialize_parameters_should_return_true',
           'test_linear_should_return_true', 'test_linear_gradient_should_return_true']

# %% ../nbs/02_tests.test_model.ipynb 1
import torch
import torch.nn.functional as f

from fastcore.utils import *
from ..core import *
from ..model import Lin, ReLU, MSE, initialize_parameters, log_softmax, nll, cross_entropy

import pytest

# %% ../nbs/02_tests.test_model.ipynb 4
A = torch.arange(start=-4, end=8, dtype=torch.float)

# %% ../nbs/02_tests.test_model.ipynb 5
A = torch.reshape(A, (4, 3))

# %% ../nbs/02_tests.test_model.ipynb 7
B = torch.arange(13, 25, dtype=torch.float)

# %% ../nbs/02_tests.test_model.ipynb 8
B = torch.reshape(B, (4, 3))

# %% ../nbs/02_tests.test_model.ipynb 12
def test_mse_should_return_true():
    
    output = MSE().forward(A, B)
    result = f.mse_loss(A, B)
    
    assert output == result

# %% ../nbs/02_tests.test_model.ipynb 13
def test_mse_gradient_should_return_true():
    pass

# %% ../nbs/02_tests.test_model.ipynb 15
@pytest.mark.xfail
def test_log_softmax_should_return_true():
    assert torch.equal(log_softmax(A), f.log_softmax(A, dim=1)) == True

# %% ../nbs/02_tests.test_model.ipynb 24
def test_nll_should_return_true():
    
    targ = torch.tensor([1, 0])
    sm_pred = torch.tensor([[0, 1, 2], [5, 0, 4]], dtype=torch.float)
    
    assert nll(sm_pred, targ) == -3.

# %% ../nbs/02_tests.test_model.ipynb 30
def test_cross_entropy_loss():
    pred = torch.tensor([[0, 1, 2], [5, 0, 4]], dtype=torch.float)
    targ = torch.tensor([2, 1])
    
    assert cross_entropy(pred, targ) == f.cross_entropy(pred, targ)

# %% ../nbs/02_tests.test_model.ipynb 33
@pytest.mark.xfail
def test_relu_should_return_true():
    
    output = ReLU().forward(A)
    result = f.relu(A) - 0.5
    assert output == result

# %% ../nbs/02_tests.test_model.ipynb 34
def test_relu_gradient_should_return_true():
    pass

# %% ../nbs/02_tests.test_model.ipynb 41
def test_intialize_parameters_should_return_true():
    
    m = 5 # number of rows
    nh = 3 # number of hidden layers
    w1, b1, w2, b2 = initialize_parameters(m=m, nh=nh)
    assert w1.shape == (m, nh)
    assert b1.shape == (nh,)
    assert w2.shape == (nh, 1)
    assert b2.shape == (1,)

# %% ../nbs/02_tests.test_model.ipynb 42
def test_linear_should_return_true():
    pass

# %% ../nbs/02_tests.test_model.ipynb 43
def test_linear_gradient_should_return_true():
    pass
