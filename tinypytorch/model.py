# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_model.ipynb.

# %% auto 0
__all__ = ['initialize_parameters', 'Model', 'Module', 'ReLU', 'MSE', 'log_softmax', 'nll', 'cross_entropy', 'CrossEntropy',
           'Lin']

# %% ../nbs/02_model.ipynb 3
import torch
from fastcore.utils import *
from fastcore.net import *
from .core import *
from .data import get_sample_data

# %% ../nbs/02_model.ipynb 12
def initialize_parameters(m, nh: "number of hidden layers"):
    # kaiming init / he init for relu
    w1 = torch.randn(m, nh)*math.sqrt(2./m)
    b1 = torch.zeros(nh)
    w2 = torch.randn(nh, 1)/math.sqrt(nh)
    b2 = torch.zeros(1)
    return w1, b1, w2, b2

# %% ../nbs/02_model.ipynb 13
class Model():
    
    def __init__(self, n_in, nh, n_out):
        self.w1, self.b1 = self.init_params(n_in, nh)
        self.w2, self.b2 = self.init_params(nh, n_out)
        self.layers = [Lin(self.w1, self.b1), ReLU(), Lin(self.w2, self.b2)]

    def __call__(self, x):
        for l in self.layers:
            # print("Model.__call__")
            # print(f"l={l}")
            x = l(x)
            # print(f"x.shape={x.shape}")
        
        return x
    
    def init_params(self, n, nh: "number of hidden layers"):
        w = torch.randn(n, nh)*math.sqrt(2./n)
        b = torch.zeros(nh)
        return w, b
    
    def backward(self):
        #self.loss.backward()
        for l in reversed(self.layers):
            l.backward()

# %% ../nbs/02_model.ipynb 15
class Module():
    def __call__(self, *args):
        self.args = args # it will call self.inp, and self.targ...
        self.out = self.forward(*args)
        return self.out
    
    def forward(self):
        raise Exception("Not implemented")
    
    def backward(self):
        self.bwd(self.out, *self.args)

# %% ../nbs/02_model.ipynb 18
class ReLU(Module):
    # def __call__(self, inp: 'input'):
    #     self.inp = inp
    #     self.out = inp.clamp_min(0.) - 0.5
    #     return self.out
    
    def forward(self, inp: 'input'):
        return inp.clamp_min(0.) - 0.5
    
    def bwd(self, out, inp):
        inp.g = (inp > 0).float() * out.g

# %% ../nbs/02_model.ipynb 21
class MSE(Module):
    
#     def __call__(self, inp: 'input', targ: 'target'):
#         self.inp = inp
#         self.targ = targ
        
#         print("MSE.forward")
#         print(f"inp.shape={inp.shape}")
#         print(f"inp.squeeze().shape={inp.squeeze(-1).shape}")
#         print(f"targ.shape={targ.shape}")
        
#         temp = (inp.squeeze() - targ)
#         print(f"temp={temp}")
        
#         return torch.pow(temp, 2).mean()
    
    def forward(self, inp, targ):
        
        # print("MSE.forward")
        # print(f"inp.shape={inp.shape}")
        # print(f"inp.squeeze().shape={inp.squeeze(-1).shape}")
        # print(f"targ.shape={targ.shape}")
        
        temp = (inp.squeeze() - targ)
        # print(f"temp={temp}")
        
        return torch.pow(temp, 2).mean()

    def bwd(self, out, inp, targ):
        inp.g = 2*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]
        
    # def backward(self):
    #     self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]

# %% ../nbs/02_model.ipynb 32
def log_softmax(x, dim=-1):
    return (x.exp()/(x.exp().sum(dim, keepdim=True))).log()

# %% ../nbs/02_model.ipynb 46
def nll(inp: 'input', targ: 'target'):
    print(f"inp.shape={inp.shape}")
    print(f"targ.shape={targ.shape}")
    return -inp[range(targ.shape[0]), targ].mean()

# %% ../nbs/02_model.ipynb 48
def cross_entropy(pred: 'prediction', targ: 'target'):
    sm_pred = log_softmax(pred)
    return nll(sm_pred, targ)

# %% ../nbs/02_model.ipynb 49
class CrossEntropy(Module):
    def __init__(self, debug=False):
        self.debug = debug
        pass

    def __call__(self, pred: 'predictions', targ: 'targets'):
        
        if self.debug == True: print(f"pred.shape={pred.shape}")
        
        sm_pred = log_softmax(pred)
        
        if self.debug == True: print(f"sm_pred.shape={sm_pred.shape}")
        
        return nll(sm_pred, targ)
    
    def bwd(self):
        pass

# %% ../nbs/02_model.ipynb 56
class Lin(Module):
    def __init__(self, w: 'weight', b: 'bias'):
        self.w, self.b = w, b
    
    # def __call__(self, inp):
    #     self.inp = inp
    #     self.out = inp @ self.w + self.b
    #     return self.out
    
    def forward(self, inp: 'input'):
        print("Lin.forward")
        print(f"inp={inp.shape}")
        print(f"w={self.w.shape}")
        print(f"b={self.b.shape}")
        
        output = inp @ self.w + self.b
        print(f"output.shape={output.shape}")
        return output
    
    def bwd(self, out, inp):
        inp.g = out.g @ self.w.t()
        # self.w.g = torch.einsum("bi,bj->ij", inp, out.g)
        self.w.g = inp.t() @ out.g
        # self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)
        self.b.g = out.g.sum(0)
        #self.w.g = torch.einsum("bi,bj->ij", self.inp, self.out.g)
        #self.b.g = out.g.sum(0)
