# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_model.ipynb.

# %% auto 0
__all__ = ['Module', 'ReLU', 'MSE', 'log_softmax', 'nll', 'cross_entropy', 'Lin', 'initialize_parameters', 'Model', 'train',
           'accuracy']

# %% ../nbs/02_model.ipynb 3
import torch
from fastcore.utils import *
from .core import *

# %% ../nbs/02_model.ipynb 6
class Module():
    def __call__(self, *args):
        self.args = args # it will call self.inp, and self.targ...
        self.out = self.forward(*args)
        return self.out
    
    def forward(self):
        raise Exception("Not implemented")
    
    def backward(self):
        self.bwd(self.out, *self.args)

# %% ../nbs/02_model.ipynb 9
class ReLU(Module):
    # def __call__(self, inp: 'input'):
    #     self.inp = inp
    #     self.out = inp.clamp_min(0.) - 0.5
    #     return self.out
    
    def forward(self, inp: 'input'):
        return inp.clamp_min(0.) - 0.5
    
    def bwd(self, out, inp):
        inp.g = (inp > 0).float() * out.g

# %% ../nbs/02_model.ipynb 12
class MSE(Module):
    
#     def __call__(self, inp: 'input', targ: 'target'):
#         self.inp = inp
#         self.targ = targ
        
#         print("MSE.forward")
#         print(f"inp.shape={inp.shape}")
#         print(f"inp.squeeze().shape={inp.squeeze(-1).shape}")
#         print(f"targ.shape={targ.shape}")
        
#         temp = (inp.squeeze() - targ)
#         print(f"temp={temp}")
        
#         return torch.pow(temp, 2).mean()
    
    def forward(self, inp, targ):
        
        # print("MSE.forward")
        # print(f"inp.shape={inp.shape}")
        # print(f"inp.squeeze().shape={inp.squeeze(-1).shape}")
        # print(f"targ.shape={targ.shape}")
        
        temp = (inp.squeeze() - targ)
        # print(f"temp={temp}")
        
        return torch.pow(temp, 2).mean()

    def bwd(self, out, inp, targ):
        inp.g = 2*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]
        
    # def backward(self):
    #     self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]

# %% ../nbs/02_model.ipynb 22
def log_softmax(x):
    return (x.exp()/(x.exp().sum(-1, keepdim=True))).log()

# %% ../nbs/02_model.ipynb 30
def nll(inp: 'input', targ: 'target'):
    return -inp[range(targ.shape[0]), targ].mean()

# %% ../nbs/02_model.ipynb 32
def cross_entropy(pred: 'prediction', targ: 'target'):
    sm_pred = log_softmax(pred)
    return nll(sm_pred, targ)

# %% ../nbs/02_model.ipynb 39
class Lin(Module):
    def __init__(self, w: 'weight', b: 'bias'):
        self.w, self.b = w, b
    
    # def __call__(self, inp):
    #     self.inp = inp
    #     self.out = inp @ self.w + self.b
    #     return self.out
    
    def forward(self, inp: 'input'):
        print("Lin.forward")
        print(f"inp={inp.shape}")
        print(f"w={self.w.shape}")
        print(f"b={self.b.shape}")
        
        output = inp @ self.w + self.b
        print(f"output.shape={output.shape}")
        return output
    
    def bwd(self, out, inp):
        inp.g = out.g @ self.w.t()
        # self.w.g = torch.einsum("bi,bj->ij", inp, out.g)
        self.w.g = inp.t() @ out.g
        # self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)
        self.b.g = out.g.sum(0)
        #self.w.g = torch.einsum("bi,bj->ij", self.inp, self.out.g)
        #self.b.g = out.g.sum(0)

# %% ../nbs/02_model.ipynb 45
def initialize_parameters(m, nh: "number of hidden layers"):
    # kaiming init / he init for relu
    w1 = torch.randn(m, nh)*math.sqrt(2./m)
    b1 = torch.zeros(nh)
    w2 = torch.randn(nh, 1)/math.sqrt(nh)
    b2 = torch.zeros(1)
    return w1, b1, w2, b2

# %% ../nbs/02_model.ipynb 46
class Model():
    def __init__(self, w1, b1, w2, b2):
        self.layers = [Lin(w1, b1), ReLU(), Lin(w2, b2)]
        self.loss = MSE()
    
    def __call__(self, x, targ):
        for l in self.layers:
            print("Model.__call__")
            print(f"l={l}")
            x = l(x)
            print(f"x.shape={x.shape}")
        
        #assert x.shape==torch.Size([targ.shape[0],1])
        
        return self.loss(x, targ)
    
    def backward(self):
        self.loss.backward()
        for l in reversed(self.layers):
            l.backward()

# %% ../nbs/02_model.ipynb 48
def train(epochs):
    pass

# %% ../nbs/02_model.ipynb 50
def accuracy(out, yb):
    return (torch.argmax(out, dim=1) == yb).float().mean()
