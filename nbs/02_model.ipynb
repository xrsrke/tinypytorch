{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: model\n",
    "output-file: model.html\n",
    "description: model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> A simple API for creating and using playing cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from fastcore.utils import *\n",
    "from fastcore.net import *\n",
    "from tinypytorch.core import *\n",
    "from tinypytorch.data import get_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xb, yb = get_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class Model():\n",
    "#     def __init__(self, m, nh: \"number of hidden\"):\n",
    "#         self.m, self.nh = m, nh\n",
    "    \n",
    "#     def forward(self, xb: 'training batch'):\n",
    "#         w1, b1, w2, b2 = self.initialize_parameters()\n",
    "#         l1 = Lin(xb, w1, b1)\n",
    "#         l2 = ReLU(l1)\n",
    "#         l3 = Lin(l2, w2, b2)\n",
    "        \n",
    "#         return l3\n",
    "    \n",
    "#     def initialize_parameters(self):\n",
    "#         # kaiming init / he init for relu\n",
    "#         w1 = torch.randn(self.m, self.nh)*math.sqrt(2./self.m)\n",
    "#         b1 = torch.zeros(self.nh)\n",
    "#         w2 = torch.randn(self.nh, 1)/math.sqrt(self.nh)\n",
    "#         b2 = torch.zeros\n",
    "#         return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class Model():\n",
    "#     def __init__(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class Model():\n",
    "#     def __init__(self, w1, b1, w2, b2):\n",
    "#         self.layers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def get_model():\n",
    "#     model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def initialize_parameters(m, nh: \"number of hidden layers\"):\n",
    "    # kaiming init / he init for relu\n",
    "    w1 = torch.randn(m, nh)*math.sqrt(2./m)\n",
    "    b1 = torch.zeros(nh)\n",
    "    w2 = torch.randn(nh, 1)/math.sqrt(nh)\n",
    "    b2 = torch.zeros(1)\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model():\n",
    "    \n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self.w1, self.b1 = self.init_params(n_in, nh)\n",
    "        self.w2, self.b2 = self.init_params(nh, n_out)\n",
    "        self.layers = [Lin(self.w1, self.b1), ReLU(), Lin(self.w2, self.b2)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            # print(\"Model.__call__\")\n",
    "            # print(f\"l={l}\")\n",
    "            x = l(x)\n",
    "            # print(f\"x.shape={x.shape}\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def init_params(self, n, nh: \"number of hidden layers\"):\n",
    "        w = torch.randn(n, nh)*math.sqrt(2./n)\n",
    "        b = torch.zeros(nh)\n",
    "        return w, b\n",
    "    \n",
    "    def backward(self):\n",
    "        #self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args # it will call self.inp, and self.targ...\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self):\n",
    "        raise Exception(\"Not implemented\")\n",
    "    \n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU(Module):\n",
    "    # def __call__(self, inp: 'input'):\n",
    "    #     self.inp = inp\n",
    "    #     self.out = inp.clamp_min(0.) - 0.5\n",
    "    #     return self.out\n",
    "    \n",
    "    def forward(self, inp: 'input'):\n",
    "        return inp.clamp_min(0.) - 0.5\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MSE(Module):\n",
    "    \n",
    "#     def __call__(self, inp: 'input', targ: 'target'):\n",
    "#         self.inp = inp\n",
    "#         self.targ = targ\n",
    "        \n",
    "#         print(\"MSE.forward\")\n",
    "#         print(f\"inp.shape={inp.shape}\")\n",
    "#         print(f\"inp.squeeze().shape={inp.squeeze(-1).shape}\")\n",
    "#         print(f\"targ.shape={targ.shape}\")\n",
    "        \n",
    "#         temp = (inp.squeeze() - targ)\n",
    "#         print(f\"temp={temp}\")\n",
    "        \n",
    "#         return torch.pow(temp, 2).mean()\n",
    "    \n",
    "    def forward(self, inp, targ):\n",
    "        \n",
    "        # print(\"MSE.forward\")\n",
    "        # print(f\"inp.shape={inp.shape}\")\n",
    "        # print(f\"inp.squeeze().shape={inp.squeeze(-1).shape}\")\n",
    "        # print(f\"targ.shape={targ.shape}\")\n",
    "        \n",
    "        temp = (inp.squeeze() - targ)\n",
    "        # print(f\"temp={temp}\")\n",
    "        \n",
    "        return torch.pow(temp, 2).mean()\n",
    "\n",
    "    def bwd(self, out, inp, targ):\n",
    "        inp.g = 2*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]\n",
    "        \n",
    "    # def backward(self):\n",
    "    #     self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.randn(size=[4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3878],\n",
       "        [-2.8296],\n",
       "        [ 1.1481],\n",
       "        [-0.3518]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_b = torch.rand_like(tensor_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0700],\n",
       "        [0.1494],\n",
       "        [0.7702],\n",
       "        [0.4558]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1673)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse.forward(tensor_a, tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy is a derivation from Kull-back divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_softmax(x, dim=-1):\n",
    "    return (x.exp()/(x.exp().sum(dim, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = log_softmax(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_near(log_softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = torch.tensor([5, 0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = torch.tensor([[0, 1, 2], [5, 0, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [5, 0, 4]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0, 1], [2, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[range(yb.shape[0]), yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nll(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nll(inp: 'input', targ: 'target'):\n",
    "    print(f\"inp.shape={inp.shape}\")\n",
    "    print(f\"targ.shape={targ.shape}\")\n",
    "    return -inp[range(targ.shape[0]), targ].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cross_entropy(pred: 'prediction', targ: 'target'):\n",
    "    sm_pred = log_softmax(pred)\n",
    "    return nll(sm_pred, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CrossEntropy(Module):\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        pass\n",
    "\n",
    "    def __call__(self, pred: 'predictions', targ: 'targets'):\n",
    "        \n",
    "        if self.debug == True: print(f\"pred.shape={pred.shape}\")\n",
    "        \n",
    "        sm_pred = log_softmax(pred)\n",
    "        \n",
    "        if self.debug == True: print(f\"sm_pred.shape={sm_pred.shape}\")\n",
    "        \n",
    "        return nll(sm_pred, targ)\n",
    "    \n",
    "    def bwd(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([[0, 1, 2], [5, 0, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = torch.tensor([2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=torch.Size([2, 3])\n",
      "targ.shape=torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.8629)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(pred, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class Lin():\n",
    "#     def __init__(self, w, b):\n",
    "#         self.w = w\n",
    "#         self.b = b\n",
    "    \n",
    "#     def __call__(self, inp):\n",
    "#         self.inp = inp\n",
    "#         self.out = inp @ self.w + self.b\n",
    "#         return self.out\n",
    "    \n",
    "#     def backward(self):\n",
    "#         self.inp.g = self.out.g @ self.w.t()\n",
    "#         self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "#         self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Lin(Module):\n",
    "    def __init__(self, w: 'weight', b: 'bias'):\n",
    "        self.w, self.b = w, b\n",
    "    \n",
    "    # def __call__(self, inp):\n",
    "    #     self.inp = inp\n",
    "    #     self.out = inp @ self.w + self.b\n",
    "    #     return self.out\n",
    "    \n",
    "    def forward(self, inp: 'input'):\n",
    "        print(\"Lin.forward\")\n",
    "        print(f\"inp={inp.shape}\")\n",
    "        print(f\"w={self.w.shape}\")\n",
    "        print(f\"b={self.b.shape}\")\n",
    "        \n",
    "        output = inp @ self.w + self.b\n",
    "        print(f\"output.shape={output.shape}\")\n",
    "        return output\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        # self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        # self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = out.g.sum(0)\n",
    "        #self.w.g = torch.einsum(\"bi,bj->ij\", self.inp, self.out.g)\n",
    "        #self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
